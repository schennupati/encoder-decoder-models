model:
  arch: encoder_decoder # Shared encoder and independent decoders
  encoder: resnet50
  decoder: DeepLabv3 #fpn or DeepLabv3
  pretrained_path: /home/sumche/results
  loss_fn: fixed # fixed, uncertainty etc
  outputs:
    semantic:
      active: &semantic_active False
      out_channels: 19
      loss: cross_entropy2d
      loss_weight: 1.0
      metric: classification_metrics
      postproc: argmax
    semantic_with_instance:
      active: &semantic_with_instance True
      out_channels: 21
      loss: dualityfocalloss
      loss_weight: 1.0
      metric: classification_metrics
      postproc: argmax
    instance_contour:
      active: &contour_active False
      out_channels: 1
      loss: weighted_binary_cross_entropy
      loss_weight: 1.0
      metric: classification_metrics # TODO: panoptic_metrics
      postproc: argmax
    instance_regression:
      active: &regression_active False
      out_channels: 2
      loss: l1
      loss_weight: 1.0
      metric: None
      postproc: None
    instance_heatmap:
      active: &heatmap_active False
      out_channels: 1
      loss: l1
      loss_weight: 1.0
      metric: None
      postproc: None
    instance_probs:
      active: &probs_active False
      out_channels: 2
      loss: cross_entropy2d
      loss_weight: 20.0
      metric: classification_metrics
      postproc: argmax

tasks: # for dataloaders
  semantic:
    active: True
    type: long
  instance:
    active: True
    type: float

postprocs:
  panoptic:
    active: True
    inputs:
      semantic:
        active: *semantic_active
      instance_contour:
        active: *contour_active
      instance_regression:
        active: *regression_active
      instance_heatmaps:
        active: *heatmap_active
      instance_probs:
        active: *probs_active
    metric: panoptic_metrics

data:
  dataset: Cityscapes
  root_path: /home/sumche/datasets/Cityscapes
  im_size: &im_size 256
  random_scaling: !!python/tuple &random_scaling [0.5, 1.0]
  val_im_size: &val_im_size 512
  transforms:
    train:
      input:
        Resize:
          flag: True
          size: *val_im_size
        RandomCrop:
          flag: True
          size: *im_size
        RandomResizedCrop:
          flag: False
          size: *im_size
          scale: *random_scaling
        RandomHorizontalFlip:
          flag: True
        ColorJitter:
          flag: False
          brightness: 0.25
          contrast: 0.25
          saturation: 0.25
          hue: 0.25
        ToTensor:
          flag: True
        Normalize:
          flag: True
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
      target:
        Resize:
          flag: True
          size: *val_im_size
        RandomCrop:
          flag: True
          size: *im_size
        RandomResizedCrop:
          flag: False
          size: *im_size
          scale: *random_scaling
        RandomHorizontalFlip:
          flag: True
        ToTensor:
          flag: True
    val:
      input:
        Resize:
          flag: True
          size: *val_im_size
        ToTensor:
          flag: True
        Normalize:
          flag: True
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
      target:
        Resize:
          flag: True
          size: *val_im_size
        ToTensor:
          flag: True

params:
  savelogs: True
  batchsize: &batch_size 24
  n_workers: &n_workers 8
  multigpu: True
  gpu_id: 1
  print_interval: 10
  resume: True
  early_stop: True
  save_criteria: metric
  patience: 25
  epochs: 100
  train:
    batch_size: *batch_size
    shuffle: True
    n_workers: *n_workers
    optimizer:
      name: adam
      lr: 1.0e-4
      amsgrad: True
      weight_decay: 1.0e-4
  val:
    batch_size: 24
    shuffle: True
    n_workers: *n_workers
